{"cells":[{"cell_type":"markdown","metadata":{},"source":["Text Summarization with NLTK in Python"]},{"cell_type":"markdown","metadata":{},"source":["Text summarization is a subdomain of Natural Language Processing (NLP) that deals with extracting summaries from huge chunks of texts. There are two main types of techniques used for text summarization: NLP-based techniques and deep learning-based techniques. In this article, we will see a simple NLP-based technique for text summarization. We will not use any machine learning library in this article. Rather we will simply use Python's NLTK library for summarizing Wikipedia articles."]},{"cell_type":"markdown","metadata":{},"source":["\n","\n"," \n"]},{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting pip\n","  Downloading pip-20.1.1-py2.py3-none-any.whl (1.5 MB)\n","\u001b[K     |████████████████████████████████| 1.5 MB 7.0 MB/s eta 0:00:01\n","\u001b[?25hInstalling collected packages: pip\n","  Attempting uninstall: pip\n","    Found existing installation: pip 20.1\n","    Uninstalling pip-20.1:\n","      Successfully uninstalled pip-20.1\n","Successfully installed pip-20.1.1\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["pip install --upgrade pip\n"]},{"cell_type":"markdown","metadata":{},"source":["**** * Fetching Articles from Wikipedia\n"," Before we could summarize Wikipedia articles, we need to fetch them from the web. To do so we will use a couple of libraries. The first library that we need to download is the beautiful soup which is very useful Python utility for web scraping. Execute the following command at the command prompt to download the Beautiful Soup utility.**********"]},{"cell_type":"code","execution_count":2,"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.7/site-packages (4.9.0)\n","Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.7/site-packages (from beautifulsoup4) (1.9.4)\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["pip install beautifulsoup4"]},{"cell_type":"code","execution_count":3,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting lxml\n","  Downloading lxml-5.3.0-cp311-cp311-win_amd64.whl.metadata (3.9 kB)\n","Downloading lxml-5.3.0-cp311-cp311-win_amd64.whl (3.8 MB)\n","   ---------------------------------------- 0.0/3.8 MB ? eta -:--:--\n","   --------------------- ------------------ 2.1/3.8 MB 10.7 MB/s eta 0:00:01\n","   ---------------------------------------- 3.8/3.8 MB 10.8 MB/s eta 0:00:00\n","Installing collected packages: lxml\n","Successfully installed lxml-5.3.0\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["pip install lxml"]},{"cell_type":"markdown","metadata":{},"source":["**Another important library that we need to parse XML and HTML is the lxml library. Execute the following command at command prompt to download lxml\n","******"]},{"cell_type":"code","execution_count":1,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: nltk in c:\\users\\god\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.8.1)\n","Requirement already satisfied: click in c:\\users\\god\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (8.1.7)\n","Requirement already satisfied: joblib in c:\\users\\god\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (1.3.2)\n","Requirement already satisfied: regex>=2021.8.3 in c:\\users\\god\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (2024.5.15)\n","Requirement already satisfied: tqdm in c:\\users\\god\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (4.66.5)\n","Requirement already satisfied: colorama in c:\\users\\god\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from click->nltk) (0.4.6)\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["pip install nltk"]},{"cell_type":"markdown","metadata":{},"source":["**NLTK is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries, and an active discussion forum.**"]},{"cell_type":"markdown","metadata":{},"source":["# Preprocessing\n","The first preprocessing step is to remove references from the article. Wikipedia, references are enclosed in square brackets. The following script removes the square brackets and replaces the resulting multiple spaces by a single space. \n","\n","# Removing Square Brackets and Extra Spaces\n","\n","The article_text object contains text without brackets. However, we do not want to remove anything else from the article since this is the original article. We will not remove other numbers, punctuation marks and special characters from this text since we will use this text to create summaries and weighted word frequencies will be replaced in this article.\n","\n","To clean the text and calculate weighted frequences, we will create another object. \n","\n","# Removing special characters and digits\n","\n","Now we have two objects article_text, which contains the original article and formatted_article_text which contains the formatted article. We will use formatted_article_text to create weighted frequency histograms for the words and will replace these weighted frequencies with the words in the article_text object.\n","\n","# Converting Text To Sentences\n","At this point we have preprocessed the data. Next, we need to tokenize the article into sentences. We will use thearticle_text object for tokenizing the article to sentence since it contains full stops. The formatted_article_text does not contain any punctuation and therefore cannot be converted into sentences using the full stop as a parameter.\n","\n","# Find Weighted Frequency of Occurrence\n","To find the frequency of occurrence of each word, we use the formatted_article_text variable. We used this variable to find the frequency of occurrence since it doesn't contain punctuation, digits, or other special characters.\n","\n","In the script above, we first store all the English stop words from the nltk library into a stopwords variable. Next, we loop through all the sentences and then corresponding words to first check if they are stop words. If not, we proceed to check whether the words exist in word_frequency dictionary i.e. word_frequencies, or not. If the word is encountered for the first time, it is added to the dictionary as a key and its value is set to 1. Otherwise, if the word previously exists in the dictionary, its value is simply updated by 1.\n","\n","Finally, to find the weighted frequency, we can simply divide the number of occurances of all the words by the frequency of the most occurring word.\n","\n","# Calculating Sentence Scores\n","We have now calculated the weighted frequencies for all the words. Now is the time to calculate the scores for each sentence by adding weighted frequencies of the words that occur in that particular sentence. \n","\n","n the script above, we first create an empty sentence_scores dictionary. The keys of this dictionary will be the sentences themselves and the values will be the corresponding scores of the sentences. Next, we loop through each sentence in the sentence_list and tokenize the sentence into words.\n","\n","We then check if the word exists in the word_frequencies dictionary. This check is performed since we created the sentence_list list from the article_text object; on the other hand, the word frequencies were calculated using the formatted_article_text object, which doesn't contain any stop words, numbers, etc.\n","\n","We do not want very long sentences in the summary, therefore, we calculate the score for only sentences with less than 30 words (although you can tweak this parameter for your own use-case). Next, we check whether the sentence exists in the sentence_scores dictionary or not. If the sentence doesn't exist, we add it to the sentence_scores dictionary as a key and assign it the weighted frequency of the first word in the sentence, as its value. On the contrary, if the sentence exists in the dictionary, we simply add the weighted frequency of the word to the existing value.\n","\n","# Getting the Summary\n","Now we have the sentence_scores dictionary that contains sentences with their corresponding score. To summarize the article, we can take top N sentences with the highest scores. The following script retrieves top 7 sentences and prints them on the screen.\n","\n","In the script above, we use the heapq library and call its nlargest function to retrieve the top 7 sentences with the highest scores."]},{"cell_type":"code","execution_count":1,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Other studies have suggested that the virus may be airborne as well, with aerosols potentially being able to transmit the virus. The host protein neuropilin 1 (NRP1) may aid the virus in host cell entry using ACE2. During the initial outbreak in Wuhan, China, various names were used for the virus; some names used by different sources included \"the coronavirus\" or \"Wuhan coronavirus\". The virus previously had the provisional name 2019 novel coronavirus (2019-nCoV), and has also been called human coronavirus 2019 (HCoV-19 or hCoV-19). Differences between the bat coronavirus and SARS‑CoV‑2 suggest that humans may have been infected via an intermediate host; although the source of introduction into humans remains unknown. The original source of viral transmission to humans remains unclear, as does whether the virus became pathogenic before or after the spillover event. Research into the natural reservoir of the virus that caused the 2002–2004 SARS outbreak has resulted in the discovery of many SARS-like bat coronaviruses, most originating in horseshoe bats.\n"]}],"source":["import bs4 as bs\n","import urllib.request\n","import re\n","import nltk\n","\n","scraped_data = urllib.request.urlopen('https://en.wikipedia.org/wiki/Severe_acute_respiratory_syndrome_coronavirus_2')\n","article = scraped_data.read()\n","\n","parsed_article = bs.BeautifulSoup(article,'lxml')\n","\n","paragraphs = parsed_article.find_all('p')\n","\n","article_text = \"\"\n","\n","for p in paragraphs:\n","    article_text += p.text\n","# Removing Square Brackets and Extra Spaces\n","article_text = re.sub(r'\\[[0-9]*\\]', ' ', article_text)\n","article_text = re.sub(r'\\s+', ' ', article_text)\n","# Removing special characters and digits\n","formatted_article_text = re.sub('[^a-zA-Z]', ' ', article_text )\n","formatted_article_text = re.sub(r'\\s+', ' ', formatted_article_text)\n","sentence_list = nltk.sent_tokenize(article_text)\n","stopwords = nltk.corpus.stopwords.words('english')\n","\n","word_frequencies = {}\n","for word in nltk.word_tokenize(formatted_article_text):\n","    if word not in stopwords:\n","        if word not in word_frequencies.keys():\n","            word_frequencies[word] = 1\n","        else:\n","            word_frequencies[word] += 1\n","    maximum_frequncy = max(word_frequencies.values())\n","for word in word_frequencies.keys():\n","    word_frequencies[word] = (word_frequencies[word]/maximum_frequncy)\n","    sentence_scores = {}\n","for sent in sentence_list:\n","    for word in nltk.word_tokenize(sent.lower()):\n","        if word in word_frequencies.keys():\n","            if len(sent.split(' ')) < 30:\n","                if sent not in sentence_scores.keys():\n","                    sentence_scores[sent] = word_frequencies[word]\n","                else:\n","                    sentence_scores[sent] += word_frequencies[word]\n","import heapq\n","summary_sentences = heapq.nlargest(7, sentence_scores, key=sentence_scores.get)\n","\n","summary = ' '.join(summary_sentences)\n","print(summary)"]},{"cell_type":"markdown","metadata":{},"source":["#### The below code is a improved version of the above code does the same thing but little optimized"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to\n","[nltk_data]     C:\\Users\\GOD\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to\n","[nltk_data]     C:\\Users\\GOD\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"name":"stdout","output_type":"stream","text":["Research into the natural reservoir of the virus that caused the 2002–2004 SARS outbreak has resulted in the discovery of many SARS-like bat coronaviruses, most originating in horseshoe bats. Studies have shown that SARS‑CoV‑2 has a higher affinity to human ACE2 than the original SARS virus. SARS‑CoV‑2 is a strain of the species Betacoronavirus pandemicum (SARSr-CoV), as is SARS-CoV-1, the virus that caused the 2002–2004 SARS outbreak. During the initial outbreak in Wuhan, China, various names were used for the virus; some names used by different sources included \"the coronavirus\" or \"Wuhan coronavirus\". Like the SARS-related coronavirus implicated in the 2003 SARS outbreak, SARS‑CoV‑2 is a member of the subgenus Sarbecovirus (beta-CoV lineage B). Other studies have suggested that the virus may be airborne as well, with aerosols potentially being able to transmit the virus. The host protein neuropilin 1 (NRP1) may aid the virus in host cell entry using ACE2.\n"]}],"source":["import requests\n","from bs4 import BeautifulSoup\n","import re\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize, sent_tokenize\n","from collections import Counter\n","import heapq\n","\n","# Download the stopwords from NLTK if not already downloaded\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","\n","# Fetch article data\n","url = 'https://en.wikipedia.org/wiki/Severe_acute_respiratory_syndrome_coronavirus_2'\n","response = requests.get(url)\n","article = response.text\n","\n","# Parse the article\n","soup = BeautifulSoup(article, 'lxml')\n","paragraphs = soup.find_all('p')\n","article_text = \" \".join([p.text for p in paragraphs])\n","\n","# Clean the text\n","article_text = re.sub(r'\\[[0-9]*\\]', '', article_text)  # Remove reference numbers\n","article_text = re.sub(r'\\s+', ' ', article_text)  # Remove extra spaces\n","\n","# Remove special characters and digits\n","formatted_article_text = re.sub('[^a-zA-Z]', ' ', article_text)\n","formatted_article_text = re.sub(r'\\s+', ' ', formatted_article_text)\n","\n","# Tokenize sentences\n","sentence_list = sent_tokenize(article_text)\n","\n","# Stopwords\n","stop_words = set(stopwords.words('english'))\n","\n","# Calculate word frequencies\n","word_frequencies = Counter(word.lower() for word in word_tokenize(formatted_article_text) if word.lower() not in stop_words)\n","\n","# Normalize frequencies\n","maximum_frequency = max(word_frequencies.values())\n","for word in word_frequencies:\n","    word_frequencies[word] /= maximum_frequency\n","\n","# Score sentences based on word frequencies\n","sentence_scores = {}\n","for sent in sentence_list:\n","    word_count = len(sent.split(' '))\n","    if word_count < 30:  # Consider only shorter sentences\n","        for word in word_tokenize(sent.lower()):\n","            if word in word_frequencies:\n","                if sent not in sentence_scores:\n","                    sentence_scores[sent] = word_frequencies[word]\n","                else:\n","                    sentence_scores[sent] += word_frequencies[word]\n","\n","# Generate summary\n","summary_sentences = heapq.nlargest(7, sentence_scores, key=sentence_scores.get)\n","summary = ' '.join(summary_sentences)\n","\n","print(summary)\n"]},{"cell_type":"markdown","metadata":{},"source":["Leverage TfidfVectorizer from Scikit-Learn: This can replace manual frequency calculations and better weigh words by their importance in the document.\n","Use NLTK's TextBlob for Sentence Tokenization: This provides a higher level of abstraction for handling text.\n","Improve Text Cleaning Using re.sub with Better Regex: Reduce redundancy in the cleaning steps."]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["[130][131]\n"," A phylogenetic tree based on whole-genome sequences of SARS-CoV-2 and related coronaviruses is:[132][133]\n"," (Bat) Rc-o319, 81% to SARS-CoV-2, Rhinolophus cornutus, Iwate, Japan[134]\n"," Bat SL-ZXC21, 88% to SARS-CoV-2, Rhinolophus pusillus, Zhoushan, Zhejiang[135]\n"," Bat SL-ZC45, 88% to SARS-CoV-2, Rhinolophus pusillus, Zhoushan, Zhejiang[135]\n"," Pangolin SARSr-CoV-GX, 85.3% to SARS-CoV-2, Manis javanica, smuggled from Southeast Asia[136]\n"," Pangolin SARSr-CoV-GD, 90.1% to SARS-CoV-2, Manis javanica, smuggled from Southeast Asia[137]\n"," Bat RshSTT182, 92.6% to SARS-CoV-2, Rhinolophus shameli, Steung Treng, Cambodia[138]\n"," Bat RshSTT200, 92.6% to SARS-CoV-2, Rhinolophus shameli, Steung Treng, Cambodia[138]\n"," (Bat) RacCS203, 91.5% to SARS-CoV-2, Rhinolophus acuminatus, Chachoengsao, Thailand[133]\n"," (Bat) RmYN02, 93.3% to SARS-CoV-2, Rhinolophus malayanus, Mengla, Yunnan[139]\n"," (Bat) RpYN06, 94.4% to SARS-CoV-2, Rhinolophus pusillus, Xishuangbanna, Yunnan[132]\n"," (Bat) RaTG13, 96.1% to SARS-CoV-2, Rhinolophus affinis, Mojiang, Yunnan[140]\n"," (Bat) BANAL-52, 96.8% to SARS-CoV-2, Rhinolophus malayanus, Vientiane, Laos[141]\n"," SARS-CoV-2\n"," SARS-CoV-1, 79% to SARS-CoV-2\n"," \n"," There are many thousands of variants of SARS-CoV-2, which can be grouped into the much larger clades. [125]\n"," On 11 February 2020, the International Committee on Taxonomy of Viruses announced that according to existing rules that compute hierarchical relationships among coronaviruses based on five conserved sequences of nucleic acids, the differences between what was then called 2019-nCoV and the virus from the 2003 SARS outbreak were insufficient to make them separate viral species. [76]\n"," The first reported case of reinfection was a 33-year-old man from Hong Kong who first tested positive on 26 March 2020, was discharged on 15 April 2020 after two negative tests, and tested positive again on 15 August 2020 (142 days later), which was confirmed by whole-genome sequencing showing that the viral genomes between the episodes belong to different clades. [86] Evidence against this hypothesis includes the fact that pangolin virus samples are too distant to SARS-CoV-2: isolates obtained from pangolins seized in Guangdong were only 92% identical in sequence to the SARS‑CoV‑2 genome (matches above 90 percent may sound high, but in genomic terms it is a wide evolutionary gap[101]). [155] The mutation of CG dinucleotides is thought to arise to avoid the zinc finger antiviral protein related defense mechanism of cells,[156] and to lower the energy to unbind the genome during replication and translation (adenosine and uracil base pair via two hydrogen bonds, cytosine and guanine via three).\n"]}],"source":["import requests\n","from bs4 import BeautifulSoup\n","import re\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from nltk.tokenize import sent_tokenize\n","import nltk\n","\n","# Download the necessary NLTK data\n","#nltk.download('punkt')\n","\n","# Fetch article data\n","url = 'https://en.wikipedia.org/wiki/Severe_acute_respiratory_syndrome_coronavirus_2'\n","response = requests.get(url)\n","article = response.text\n","\n","# Parse the article\n","soup = BeautifulSoup(article, 'lxml')\n","paragraphs = soup.find_all('p')\n","article_text = \" \".join([p.text for p in paragraphs])\n","\n","# Clean the text\n","def clean_text(text):\n","    clean_text = re.sub(r'\\[[0-9]*\\]', '', text)  # Remove reference numbers\n","    clean_text = re.sub(r'\\s+', ' ', clean_text)  # Remove extra spaces\n","    clean_text = re.sub('[^a-zA-Z]', ' ', clean_text)  # Keep only alphabets\n","    return clean_text.strip()\n","\n","cleaned_article = clean_text(article_text)\n","\n","# Tokenize sentences\n","sentences = sent_tokenize(article_text)\n","\n","# Generate TF-IDF matrix\n","vectorizer = TfidfVectorizer(stop_words='english')\n","tfidf_matrix = vectorizer.fit_transform(sentences)\n","\n","# Calculate sentence scores based on TF-IDF\n","sentence_scores = tfidf_matrix.sum(axis=1).flatten().tolist()[0]\n","\n","# Get the top N sentences\n","def summarize(sentences, scores, top_n=7):\n","    ranked_sentences = sorted(((scores[i], s) for i, s in enumerate(sentences)), reverse=True)\n","    summary = ' '.join([ranked_sentences[i][1] for i in range(top_n)])\n","    return summary\n","\n","# Generate summary\n","summary = summarize(sentences, sentence_scores, top_n=20)\n","print(summary)\n"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["In the early period of the loan tenure, the EMI will have a higher interest component and lower principal amount, but this will reverse as you near the end stages.HDFC Bank offers loan amounts upto Rs. HDFC Bank usually disburses loan within 10 seconds if you are a pre-approved customer, while non-HDFC Bank customers can get the loan in 4 days.What other options do I have apart from Personal Loans?If you are not sure about a Personal Loan, then HDFC Bank offers several other options that you can use to generate funds for your needs. Just like most loans, however, it must be repaid in monthly instalments.You can use it to fund any expense including education, a wedding, a trip, home renovation, medical expenses, and even to buy a gadget. You can even use the money to help out with the day-to-day expenses in case of a cash flow crunch.HDFC Bank offers a Personal Loan to pre-approved customers in just 10 seconds. This instalment amount is calculated using the loan amount, the payment tenure, and the interest rate. A Car Loan or a Home Loan must be used for that specific purpose but the Personal Loan can be used for any purpose like vocational courses, home renovation, medical emergency, or to travel , just about anything. There are three factors that determine your EMI: The easiest way to calculate EMI is to use online calculators like the HDFC Bank Personal Loan EMI calculator. What are the uses of a Personal Loan?There are several ways in which a Personal Loan can be used: You don’t need to blow all your savings on your dream vacation trip. It is the intervallic instalment amount that you pay to clear your loan.It is important to calculate your EMI and find a way to keep it as low as possible. As an already existing customer, you can apply via NetBanking on the HDFC Bank website, through an ATM or Loan Assist App. 40 lac, for a tenure ranging between 12 and 60 months, with EMI as low as Rs. How to Check Eligibility for Personal LoanYou need to be sure that you are eligible for a Personal Loan, before you consider applying for one. You can apply for a HDFC Bank personal loan online, through an ATM, Loan Assist App or personally at the bank. Otherwise, you can drop by the nearest branch to get the process started.You can get a re-payment tenure that suits your needs. Personal Loan does not require any collateral or security and can be obtained with minimal documentation. You can use HDFC Bank Personal Loan eligibility calculator to determine how much you can borrow. Credit Card approvals is subject to documentation and verification as per Banks requirement. Time, urgency and capacity to repay are important factors you need to consider. 1878 per lac.​​​​​​​ How do I apply for a Personal Loan?Applying for a Personal Loan is quick and easy. Credit Card approvals at the sole discretion of HDFC Bank limited.\n"]}],"source":["import requests\n","from bs4 import BeautifulSoup\n","import re\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from nltk.tokenize import sent_tokenize\n","import nltk\n","\n","# Download necessary NLTK data\n","#nltk.download('punkt')\n","\n","def fetch_text_from_url(url):\n","    \"\"\"\n","    Fetches text from a given URL.\n","\n","    Args:\n","        url (str): URL of the webpage to scrape.\n","\n","    Returns:\n","        str: Raw text extracted from the webpage.\n","    \"\"\"\n","    response = requests.get(url)\n","    article = response.text\n","    soup = BeautifulSoup(article, 'lxml')\n","    paragraphs = soup.find_all('p')\n","    return \" \".join([p.text for p in paragraphs])\n","\n","def clean_text(text):\n","    \"\"\"\n","    Cleans the input text by removing references, extra spaces, and non-alphabet characters.\n","\n","    Args:\n","        text (str): The raw text to be cleaned.\n","\n","    Returns:\n","        str: Cleaned text.\n","    \"\"\"\n","    text = re.sub(r'\\[[0-9]*\\]', '', text)  # Remove reference numbers\n","    text = re.sub(r'\\s+', ' ', text)  # Remove extra spaces\n","    text = re.sub('[^a-zA-Z]', ' ', text)  # Keep only alphabets\n","    return text\n","\n","def extract_sentences_with_keywords(text, keywords):\n","    \"\"\"\n","    Extracts sentences that contain specific keywords.\n","\n","    Args:\n","        text (str): The text to search for keywords.\n","        keywords (set): A set of keywords to look for in the text.\n","\n","    Returns:\n","        list: A list of sentences that contain the keywords.\n","    \"\"\"\n","    sentences = sent_tokenize(text)\n","    keyword_sentences = [sentence for sentence in sentences if any(keyword.lower() in sentence.lower() for keyword in keywords)]\n","    return keyword_sentences\n","\n","def extractive_summary_with_keywords(text, keywords, num_sentences=7):\n","    \"\"\"\n","    Generates an extractive summary from the given text using TF-IDF and keyword boosting.\n","\n","    Args:\n","        text (str): The text to summarize.\n","        keywords (set): A set of keywords to prioritize in the summary.\n","        num_sentences (int): Number of sentences to include in the summary.\n","\n","    Returns:\n","        str: Extractive summary focusing on keywords.\n","    \"\"\"\n","    # Clean and tokenize sentences\n","    cleaned_text = clean_text(text)\n","    sentences = sent_tokenize(text)\n","    \n","    # Generate TF-IDF matrix for sentences\n","    vectorizer = TfidfVectorizer(stop_words='english')\n","    tfidf_matrix = vectorizer.fit_transform(sentences)\n","\n","    # Calculate sentence scores based on TF-IDF\n","    sentence_scores = tfidf_matrix.sum(axis=1).flatten().tolist()[0]\n","    \n","    # Boost scores for sentences containing keywords\n","    keyword_sentences = extract_sentences_with_keywords(text, keywords)\n","    for i, sentence in enumerate(sentences):\n","        if sentence in keyword_sentences:\n","            sentence_scores[i] *= 1.5  # Boost score by 50% if the sentence contains a keyword\n","\n","    # Get top N sentences for the summary\n","    ranked_sentences = sorted(((sentence_scores[i], s) for i, s in enumerate(sentences)), reverse=True)\n","    summary = ' '.join([ranked_sentences[i][1] for i in range(min(num_sentences, len(ranked_sentences)))])\n","\n","    return summary\n","\n","def summarize_policy_from_url_with_keywords(url, keywords, num_sentences=7):\n","    \"\"\"\n","    Fetches a policy document from a URL and generates an extractive summary focused on specific keywords.\n","\n","    Args:\n","        url (str): URL of the policy document.\n","        keywords (set): A set of keywords to prioritize in the summary.\n","        num_sentences (int): Number of sentences to include in the summary.\n","\n","    Returns:\n","        str: Extractive summary of the policy document focusing on keywords.\n","    \"\"\"\n","    raw_text = fetch_text_from_url(url)\n","    summary = extractive_summary_with_keywords(raw_text, keywords, num_sentences=num_sentences)\n","    return summary\n","\n","# Example usage\n","policy_url = 'https://www.hdfcbank.com/personal/resources/learning-centre/borrow/everything-you-need-to-know-about-a-personal-loan'\n","keywords = {'interest', 'ROI', 'principal', 'policy', 'insurance'}\n","summary = summarize_policy_from_url_with_keywords(policy_url, keywords, num_sentences=20)\n","print(summary)\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.5"}},"nbformat":4,"nbformat_minor":4}
